# Talos Linux Setup and Config



## Installation from scratch (new secrets)

First generate a secrets file. This seems to encapsulate all the generated randomness
at startup, such that you can reuse a talosconfig or kubeconfig file across iterations
as long as they all import the same secrets file.

Master copy of the secrets.yaml is in 1password; `talos/install.sh` extracts it temporarily.

Note that all three files generated by `talosctl gen config` (worker.yaml, controlplane.yaml,
and talosconfig) contain some or all of the secrets; the external secrets file manages
the randomness but does NOT make it safe to version control the resulting files. They should
be completely reproducible given the same command line and patches, along with the external secrets,
but I keep the expanded templates in 1password just in case. (Also handled by the install script.)

### Control node in Talos

* Bring up the control node booted from the CD image.
* Once it's started, edit the boot source to select Virtual HD. (You'll have to do this at some point, might as well while you're here.)

From the top of the repo, I ran

```
./talos/install.sh
talosctl apply-config --insecure --nodes 10.0.1.50 --file controlplane.yaml
rm controlplane.yaml
```

I connected to the VM and watched the logs while it incrementally brought things up, until it asks you to bootstrap the
new etcd cluster.

```
talosctl bootstrap -n 10.0.1.50
```

This will grind a little longer and then sit there complaining about a timeout and a permission denied, but the real problem
is that it doesn't have a CNI so it can't connect to itself.

### Initial Kubernetes config

```
talosctl kubeconfig -n 10.0.1.50
```

If things are far enough along, this will save or update an entry in your `~/.kube/config` file that gives you the keys to access
the Kubernetes API in the new cluster. (These keys are stable as long as you use the same secrets.yaml file, it seems, but update
it just in case.) Use `kubectl get nodes` or something to confirm you can talk to the API.

I don't want to have to maintain two versions of a `values.yaml` file for Cilium. However, that includes creating some Prometheus monitors
which depend on `prometheus-operator` being installed. There isn't an isolated Helm chart for it any longer, they recommend `kube-prom`
(jsonnet installation of a whole monitoring stack, including prometheus-operator) or a community-maintained Helm clone of that effort. Given that
choice, I want to do `kube-prom` but I don't want to have to install it manually before I even have a CNI. But we can install just the CRDs,
straight from their github repo: `./prometheus-operator-crd/install.sh` (which just makes a series of calls to `kubectl apply`).

## Install Cilium

Now we can install cilium:  `./cilium/install.sh`

This installs a small Helm chart of our own, coincidentally named cilium, which exists to install its singular dependency (the actual cilium
Helm chart). This pattern comes from ArgoCD as a way to reuse an external Helm chart with some values overrides. This might require having
run `helm dep update` in the `tales/cilium` dir first, to pull the external dependency locally.

### Reboot

At this point the core of the control plane is complete. Stop for a moment to shut it down and reboot it with the installed configuration.
Synology's shutdown will work well now, since Talos handles the ACPI event, even though Synology will be worried about it.
Alternatively `talosctl shutdown --wait -n 10.0.1.50` should do the trick.

Just running `talosctl reboot` does NOT have the same effect, primarily that it uses `kexec` to restart the kernel and therefore does not
respect the `vga=` arg that we painstakingly added to get a usable console dashboard. But also it is a good chance to confirm that it will
in fact boot correctly off the Virtual Hard Drive and come back up healthy.

### Worker

The worker is simpler because it does not have to deal with the two-step dance to install Cilium. 

```
talosctl apply-config --insecure --nodes 10.0.1.100 --file worker.yaml
```

Once that applies, do the same shutdown to apply kernel args:

```
talosctl shutdown --wait -n 10.0.1.100
```

Once you restart the VM, it should be green (except for Secure Boot, which, meh).


### Validate installation

```
cilium status --wait -n cilium
kubectl create namespace cilium-test-1 \
  && kubectl label namespace cilium-test-1 pod-security.kubernetes.io/enforce=privileged \
  && kubectl label namespace cilium-test-1 pod-security.kubernetes.io/warn=privileged
cilium connectivity test -n cilium
```

Current issues:

* TODO: ``
* TODO: `msg="Failed to send gratuitous arp" error="failed to craft ARP reply packet: invalid IPv4 address" ipAddr="invalid IP" k8sPodName= subsys=l2-pod-announcements-garp`

```
# Fails due to single node.
talosctl conformance kubernetes -n 10.0.1.50
```
